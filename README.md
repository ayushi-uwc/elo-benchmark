# ğŸ† Clinical Reasoning Language Model Elo Rating

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![MongoDB](https://img.shields.io/badge/Database-MongoDB-green.svg)](https://www.mongodb.com/)
[![LiteLLM](https://img.shields.io/badge/LLM-LiteLLM-orange.svg)](https://github.com/BerriAI/litellm)
[![arXiv](https://img.shields.io/badge/arXiv-2024.xxxxx-b31b1b.svg)](https://arxiv.org/abs/2024.xxxxx)

*A peer-federated evaluation framework addressing benchmark overfitting and cost-efficiency trade-offs in large language model assessment*

[ğŸ“ˆ Results](#-results) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ”§ Installation](#-installation)

</div>

## ğŸ”— Benchmarks by Domain

> ### ğŸ©º for Clinical Reasoning  
> [**ğŸ”— GitHub Repository**](https://github.com/ayushi-uwc/elo-benchmark)  
> The original UNER implementation focused on diagnostic case evaluation and clinical reasoning.  
> Uses dynamic peer-generated challenges and dual-track Elo to evaluate medical knowledge, logical consistency, and decision-making accuracy.

> ### ğŸ’» for Programming Tasks  
> [**ğŸ”— GitHub Repository**](https://github.com/ayushi-uwc/elo-benchmark-programming)  
> Evaluates LLMs on algorithm design, code efficiency, debugging, and optimization skills.  
> Prompts are adapted to competitive programming-style tasks with multi-provider peer evaluation.

> ### ğŸ§® for Mathematical Reasoning  
> [**ğŸ”— GitHub Repository**](https://github.com/ayushi-uwc/elo-benchmark-mathematical-reasoning)  
> Focuses on proof construction, logical deduction, and symbolic reasoning.  
> Uses math-focused prompts and peer-reviewed challenge generation to assess core reasoning capabilities.

All benchmarks follow a standardized Swiss-pairing system with full cost-performance analysis.  
Each repo includes open logs, match histories, and leaderboard tracking.

```
DETAILED LEADERBOARD
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # â”‚Model                         â”‚Raw ELO â”‚Cost ELOâ”‚Raw Avg   â”‚Cost Avg  â”‚W-L-D       â”‚Tokens  â”‚Cost $     â”‚Matches â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 â”‚Gemini 2.5 Pro               â”‚ 1603.9 â”‚ 1561.9 â”‚   0.6989 â”‚   0.6142 â”‚ 165-69-13  â”‚4717145 â”‚ $37.75128 â”‚     50 â”‚
â”‚ 2 â”‚GPT-4.1                       â”‚ 1601.2 â”‚ 1603.7 â”‚   0.7121 â”‚   0.7183 â”‚ 169-64-13  â”‚1574472 â”‚  $5.58125 â”‚     50 â”‚
â”‚ 3 â”‚GPT-4.1 mini                  â”‚ 1599.6 â”‚ 1605.7 â”‚   0.7336 â”‚   0.7462 â”‚ 160-54-14  â”‚1551438 â”‚  $1.10994 â”‚     46 â”‚
â”‚ 4 â”‚GPT-o4-mini                   â”‚ 1568.2 â”‚ 1569.9 â”‚   0.6024 â”‚   0.6065 â”‚ 139-96-12  â”‚1884003 â”‚  $4.49846 â”‚     50 â”‚
â”‚ 5 â”‚Qwen 3.2 235B                 â”‚ 1555.4 â”‚ 1564.8 â”‚   0.6912 â”‚   0.7115 â”‚ 164-68-15  â”‚1652646 â”‚  $0.05886 â”‚     50 â”‚
â”‚ 6 â”‚Grok 3 Mini Fast              â”‚ 1537.7 â”‚ 1540.9 â”‚   0.5529 â”‚   0.5557 â”‚ 125-101-16 â”‚  986629 â”‚  $0.83581 â”‚     50 â”‚
â”‚ 7 â”‚GPT-o3-mini                   â”‚ 1533.7 â”‚ 1492.4 â”‚   0.5767 â”‚   0.4891 â”‚ 131-102-14 â”‚  792504 â”‚ $16.67280 â”‚     50 â”‚
â”‚ 8 â”‚Claude 3.7 Sonnet             â”‚ 1530.5 â”‚ 1527.1 â”‚   0.5877 â”‚   0.5823 â”‚ 139-92-16  â”‚1339119 â”‚  $8.02990 â”‚     50 â”‚
â”‚ 9 â”‚Grok 3                        â”‚ 1523.4 â”‚ 1523.2 â”‚   0.5663 â”‚   0.5659 â”‚ 128-98-21  â”‚1104597 â”‚  $8.04717 â”‚     50 â”‚
â”‚10 â”‚Qwen 3 32B                    â”‚ 1520.2 â”‚ 1537.7 â”‚   0.5521 â”‚   0.5871 â”‚ 131-104-10 â”‚2052475 â”‚  $0.07149 â”‚     50 â”‚
â”‚11 â”‚Gemini 2.0 Flash             â”‚ 1519.6 â”‚ 1527.6 â”‚   0.5118 â”‚   0.5319 â”‚ 120-114-11 â”‚  170242 â”‚  $0.02618 â”‚     50 â”‚
â”‚12 â”‚Grok 3 Fast                   â”‚ 1519.2 â”‚ 1514.7 â”‚   0.5331 â”‚   0.5271 â”‚ 123-104-19 â”‚1012959 â”‚ $12.38477 â”‚     50 â”‚
â”‚13 â”‚GPT-4o                        â”‚ 1515.3 â”‚ 1510.4 â”‚   0.5497 â”‚   0.5360 â”‚ 134-107-8  â”‚  256291 â”‚  $1.90124 â”‚     50 â”‚
â”‚14 â”‚Claude 3.5 Haiku             â”‚ 1509.8 â”‚ 1515.0 â”‚   0.5550 â”‚   0.5661 â”‚ 128-99-21  â”‚  954069 â”‚  $1.25208 â”‚     50 â”‚
â”‚15 â”‚Claude 3.5 Sonnet            â”‚ 1509.7 â”‚ 1505.6 â”‚   0.5484 â”‚   0.5366 â”‚ 126-102-19 â”‚  207043 â”‚  $1.06406 â”‚     50 â”‚
â”‚16 â”‚GPT-o3                        â”‚ 1508.4 â”‚ 1486.2 â”‚   0.9025 â”‚   0.8481 â”‚ 217-19-11 â”‚1274491 â”‚ $30.49204 â”‚     50 â”‚
â”‚17 â”‚Gemini 2.5 Flash             â”‚ 1505.6 â”‚ 1510.3 â”‚   0.5078 â”‚   0.5204 â”‚ 117-115-16 â”‚  533934 â”‚  $0.22392 â”‚     50 â”‚
â”‚18 â”‚Claude 3 Opus                â”‚ 1501.9 â”‚ 1464.9 â”‚   0.5325 â”‚   0.4507 â”‚ 124-111-10 â”‚   98088 â”‚  $2.32572 â”‚     50 â”‚
â”‚19 â”‚Gemini 2.0 Flash Lite        â”‚ 1497.3 â”‚ 1503.0 â”‚   0.4946 â”‚   0.5081 â”‚ 111-120-11 â”‚   86055 â”‚  $0.00929 â”‚     50 â”‚
â”‚20 â”‚Meta Llama 4 Scout Instruct  â”‚ 1494.6 â”‚ 1500.6 â”‚   0.5234 â”‚   0.5360 â”‚ 125-112-11 â”‚  433948 â”‚  $0.03559 â”‚     50 â”‚
â”‚21 â”‚Grok 3 Mini                   â”‚ 1493.4 â”‚ 1501.6 â”‚   0.4752 â”‚   0.4916 â”‚ 110-119-16 â”‚  336951 â”‚  $0.07963 â”‚     50 â”‚
â”‚22 â”‚Command R 7B                  â”‚ 1491.7 â”‚ 1501.2 â”‚   0.5605 â”‚   0.5819 â”‚ 127-96-19  â”‚  340480 â”‚  $0.01975 â”‚     50 â”‚
â”‚23 â”‚GPT-4.1 nano                 â”‚ 1490.4 â”‚ 1498.6 â”‚   0.4758 â”‚   0.4935 â”‚ 115-125-9  â”‚   82535 â”‚  $0.01248 â”‚     50 â”‚
â”‚24 â”‚DeepSeek R1 Distill Llama 70Bâ”‚ 1487.9 â”‚ 1499.0 â”‚   0.4588 â”‚   0.4836 â”‚ 107-125-16 â”‚  112325 â”‚  $0.09599 â”‚     50 â”‚
â”‚25 â”‚Meta Llama 4 Maverick Instructâ”‚ 1478.8 â”‚ 1483.8 â”‚   0.4667 â”‚   0.4766 â”‚ 108-128-10 â”‚  171731 â”‚  $0.01476 â”‚     50 â”‚
â”‚26 â”‚Gemma 3 27B                   â”‚ 1477.6 â”‚ 1484.5 â”‚   0.4686 â”‚   0.4845 â”‚ 108-124-14 â”‚  244013 â”‚  $0.02440 â”‚     50 â”‚
â”‚27 â”‚Microsoft Phi 4               â”‚ 1470.6 â”‚ 1472.6 â”‚   0.4646 â”‚   0.4689 â”‚ 108-126-11 â”‚   69972 â”‚  $0.00201 â”‚     50 â”‚
â”‚28 â”‚Claude 3 Sonnet              â”‚ 1470.6 â”‚ 1464.5 â”‚   0.4578 â”‚   0.4481 â”‚ 102-122-23 â”‚  151151 â”‚  $0.72019 â”‚     50 â”‚
â”‚29 â”‚Grok 2                        â”‚ 1468.8 â”‚ 1468.1 â”‚   0.4701 â”‚   0.4681 â”‚ 110-125-13 â”‚  472402 â”‚  $1.80693 â”‚     50 â”‚
â”‚30 â”‚Command A                     â”‚ 1462.8 â”‚ 1465.0 â”‚   0.6298 â”‚   0.6309 â”‚ 145-80-20  â”‚  729942 â”‚  $2.87647 â”‚     50 â”‚
â”‚31 â”‚Command R                     â”‚ 1462.2 â”‚ 1466.3 â”‚   0.5691 â”‚   0.5786 â”‚ 131-95-18  â”‚  432739 â”‚  $0.09936 â”‚     50 â”‚
â”‚32 â”‚Gemini 1.5 Flash             â”‚ 1460.6 â”‚ 1463.4 â”‚   0.4345 â”‚   0.4424 â”‚ 106-134-5  â”‚   68477 â”‚  $0.00809 â”‚     50 â”‚
â”‚33 â”‚Command R+                    â”‚ 1460.5 â”‚ 1459.4 â”‚   0.5190 â”‚   0.5136 â”‚ 120-107-18 â”‚  238925 â”‚  $1.25400 â”‚     50 â”‚
â”‚34 â”‚Gemini 1.5 Pro               â”‚ 1457.3 â”‚ 1460.8 â”‚   0.4503 â”‚   0.4569 â”‚  99-125-19 â”‚  244863 â”‚  $0.41000 â”‚     50 â”‚
â”‚35 â”‚Microsoft Phi 3.5 Mini Instructâ”‚ 1450.4 â”‚ 1457.5 â”‚   0.4333 â”‚   0.4433 â”‚ 103-132-6  â”‚   87912 â”‚  $0.00258 â”‚     50 â”‚
â”‚36 â”‚Gemma 3 12B                   â”‚ 1447.8 â”‚ 1451.3 â”‚   0.4179 â”‚   0.4250 â”‚  92-133-20 â”‚   60966 â”‚  $0.00000 â”‚     50 â”‚
â”‚37 â”‚Mistral 8x7B Instruct        â”‚ 1447.5 â”‚ 1451.1 â”‚   0.4080 â”‚   0.4199 â”‚  97-141-9  â”‚   95290 â”‚  $0.00287 â”‚     50 â”‚
â”‚38 â”‚Llama 3.3 70B                 â”‚ 1444.9 â”‚ 1444.7 â”‚   0.4239 â”‚   0.4244 â”‚  96-134-17 â”‚  116514 â”‚  $0.07293 â”‚     50 â”‚
â”‚39 â”‚Claude 3 Haiku               â”‚ 1434.1 â”‚ 1436.6 â”‚   0.4122 â”‚   0.4173 â”‚  92-139-16 â”‚   81304 â”‚  $0.03100 â”‚     50 â”‚
â”‚40 â”‚Gemini 1.5 Flash 8B          â”‚ 1426.9 â”‚ 1430.3 â”‚   0.3882 â”‚   0.3946 â”‚  90-148-9  â”‚   70439 â”‚  $0.00379 â”‚     50 â”‚
â”‚41 â”‚Gemma 3 4B                    â”‚ 1424.1 â”‚ 1428.0 â”‚   0.3976 â”‚   0.4075 â”‚  97-148-5  â”‚   71491 â”‚  $0.00000 â”‚     50 â”‚
â”‚42 â”‚LLaMA 3.1 8B Instant         â”‚ 1423.9 â”‚ 1426.5 â”‚   0.3859 â”‚   0.3901 â”‚  92-148-6  â”‚   75627 â”‚  $0.00426 â”‚     50 â”‚
â”‚43 â”‚Mistral Saba 24B             â”‚ 1421.0 â”‚ 1422.8 â”‚   0.4101 â”‚   0.4139 â”‚  99-138-10 â”‚  124411 â”‚  $0.09828 â”‚     50 â”‚
â”‚44 â”‚GPT-3.5 Turbo                â”‚ 1413.9 â”‚ 1415.7 â”‚   0.3655 â”‚   0.3761 â”‚  85-146-16 â”‚   78178 â”‚  $0.04828 â”‚     50 â”‚
â”‚45 â”‚Gemma 2 9B                    â”‚ 1411.0 â”‚ 1412.4 â”‚   0.3811 â”‚   0.3875 â”‚  87-146-8 â”‚  106920 â”‚  $0.02138 â”‚     50 â”‚
â”‚46 â”‚Gemma 3 1B                    â”‚ 1387.5 â”‚ 1388.5 â”‚   0.3470 â”‚   0.3477 â”‚  80-156-12 â”‚   84855 â”‚  $0.00000 â”‚     50 â”‚
â”‚47 â”‚Allamanda 2 7B                â”‚ 1353.9 â”‚ 1354.6 â”‚   0.3068 â”‚   0.3076 â”‚  71-167-6  â”‚  121480 â”‚  $0.01286 â”‚     50 â”‚
â”‚48 â”‚Mistral Nemo Instruct 2407   â”‚ 1274.9 â”‚ 1276.0 â”‚   0.1076 â”‚   0.1082 â”‚  17-212-18 â”‚    1558 â”‚  $0.00039 â”‚     50 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

# Elo Score Distributions for Participating Models

| Model                              | Cost Adjusted Elo | Adj Elo Rank | Raw Elo | Raw Elo Rank | Rank Difference |
|------------------------------------|-------------------|--------------|---------|--------------|-----------------|
| GPT-4.1 mini                       | 1605.7            | 1            | 1599.6  | 3            | 2               |
| GPT-4.1                            | 1603.7            | 2            | 1601.2  | 2            | 0               |
| GPT-04-mini                        | 1569.9            | 3            | 1568.2  | 4            | 1               |
| Qwen 3.2 235B                      | 1564.8            | 4            | 1555.4  | 5            | 1               |
| Gemini 2.5 Pro                     | 1561.9            | 5            | 1603.9  | 1            | -4              |
| Grok 3 Mini Fast                   | 1540.9            | 6            | 1537.7  | 6            | 0               |
| Qwen 3 32B                         | 1537.7            | 7            | 1529.9  | 10           | 3               |
| Gemini 2.0 Flash                   | 1527.6            | 8            | 1519.6  | 11           | 3               |
| Claude 3.7 Sonnet                  | 1527.1            | 9            | 1530.5  | 8            | -1              |
| Grok 3                             | 1523.2            | 10           | 1523.4  | 9            | -1              |
| Claude 3.5 Haiku                   | 1515.0            | 11           | 1509.8  | 14           | 3               |
| Grok 3 Fast                        | 1514.7            | 12           | 1519.2  | 12           | 0               |
| GPT-4o                             | 1510.4            | 13           | 1515.3  | 13           | 0               |
| Gemini 2.5 Flash                   | 1510.3            | 14           | 1506.6  | 17           | 3               |
| Claude 3.5 Sonnet                  | 1505.6            | 15           | 1509.7  | 15           | 0               |
| Gemini 2.0 Flash Lite              | 1503.0            | 16           | 1497.3  | 19           | 3               |
| Grok 3 Mini                        | 1501.6            | 17           | 1493.4  | 21           | 4               |
| Command R 7B                       | 1501.2            | 18           | 1491.7  | 22           | 4               |
| Meta Llama 4 Scout Instruct 17B    | 1500.6            | 19           | 1494.6  | 20           | 1               |
| DeepSeek R1 Distill Llama 70B      | 1499.0            | 20           | 1487.9  | 24           | 4               |
| GPT-4.1 nano                       | 1498.6            | 21           | 1490.4  | 23           | 2               |
| GPT-03-mini                        | 1492.4            | 22           | 1533.7  | 7            | -15             |
| GPT-03                             | 1486.2            | 23           | 1508.4  | 16           | -7              |
| Gemma 3 27B                        | 1484.5            | 24           | 1477.6  | 26           | 2               |
| Meta Llama 4 Maverick Instruct 17B | 1483.8            | 25           | 1478.8  | 25           | 0               |
| Microsoft Phi 2                    | 1472.6            | 26           | 1470.6  | 27           | 1               |
| Grok 2                             | 1468.1            | 27           | 1468.8  | 29           | 2               |
| Command R                          | 1466.3            | 28           | 1462.2  | 31           | 3               |
| Command A                          | 1465.0            | 29           | 1462.8  | 30           | 1               |
| Claude 3 Opus                      | 1464.9            | 30           | 1501.9  | 18           | -12             |
| Claude 3 Sonnet                    | 1464.5            | 31           | 1470.6  | 28           | -3              |
| Gemini 1.5 Flash                   | 1463.4            | 32           | 1460.6  | 32           | 0               |
| Gemini 1.5 Pro                     | 1460.8            | 33           | 1457.3  | 34           | 1               |
| Command R+                         | 1459.4            | 34           | 1460.5  | 33           | -1              |
| Microsoft Phi 3.5 Mini Instruct    | 1457.5            | 35           | 1450.4  | 35           | 0               |
| Gemma 3 12B                        | 1451.3            | 36           | 1447.8  | 36           | 0               |
| Mistral 8x7B Instruct              | 1451.1            | 37           | 1447.5  | 37           | 0               |
| Llama 3.3 70B                      | 1444.7            | 38           | 1444.9  | 38           | 0               |
| Claude 3 Haiku                     | 1436.6            | 39           | 1434.1  | 39           | 0               |
| Gemini 1.5 Flash 8B                | 1430.3            | 40           | 1426.9  | 40           | 0               |
| Gemma 3 4B                         | 1428.0            | 41           | 1424.1  | 41           | 0               |
| LLAMA 3.1 8B Instant               | 1426.5            | 42           | 1423.9  | 42           | 0               |
| Mistral Saba 24B                   | 1422.8            | 43           | 1421.0  | 43           | 0               |
| GPT-3.5 Turbo                      | 1415.7            | 44           | 1413.9  | 44           | 0               |
| Gemma 2 9B                         | 1412.4            | 45           | 1411.0  | 45           | 0               |
| Gemma 3 1B                         | 1388.5            | 46           | 1387.5  | 46           | 0               |
| Allamanda 2 7B                     | 1354.6            | 47           | 1353.9  | 47           | 0               |
| Mistral Nemo Instruct 2407         | 1276.0            | 48           | 1274.9  | 48           | 0               |

Detailed Logs have been uploaded to google drive

[Detailed Logs](https://drive.google.com/drive/folders/1vZh4me-VpUhWxts7PyF3jnm0NJ-lGL_O?usp=sharing)

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- MongoDB database
- API keys for LLM providers

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys and MongoDB URI
```

### Run Your First Tournament

```bash
# Start a tournament with default settings
python main.py

# Run with custom parameters
python main.py --batch-size 10 --stats

# Check model health
python check_models.py
```

## ğŸ› ï¸ Installation

### Environment Setup

1. **Clone Repository**
```bash
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark
```

2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure Environment**
Create a `.env` file:
```env
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_API_KEY=your_google_key
HUGGINGFACE_API_KEY=your_huggingface_key
GROQ_API_KEY=your_groq_key
XAI_API_KEY=your_xai_key

# Database Configuration
MONGODB_URI=your_mongodb_connection_string
```

### Model Configuration

Add new models in `model_definitions.py`:

```python
{
    "name": "GPT-4 Turbo",
    "model_id": "gpt-4-turbo-preview",
    "provider": "openai",
    "input_cost_per_million": 10.0,
    "output_cost_per_million": 30.0,
    "pricing_source": "OpenAI API Pricing"
}
```

## ğŸ“Š Usage

### Command Line Interface

```bash
# Basic tournament run
python main.py

# Advanced options
python main.py \
    --max-matches 100 \
    --batch-size 10 \
    --stats \
    --log-level INFO
```

### Programmatic Usage

```python
from models import initialize_models
from tournament import run_tournament_matches
from model_definitions import MODELS

# Initialize models
models = initialize_models(MODELS)

# Run tournament
matches = run_tournament_matches(models, max_matches=50)

# Analyze results
for match in matches:
    print(f"Match: {match.participants}")
    print(f"Scores: {match.judgment['raw_score']}")
```

### Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `max_matches` | 50 | Maximum matches per model |
| `batch_size` | 10 | Matches per batch |
| `k_factor` | 32 | Elo update rate |
| `cost_temperature` | 0.05 | Cost sensitivity |
| `judge_temperature` | 300 | Judge weight temperature |

## ğŸ“ Project Structure

```
elo-benchmark/
â”œâ”€â”€ ğŸ“„ main.py                    # Main entry point
â”œâ”€â”€ ğŸ† tournament.py              # Tournament management and pairing
â”œâ”€â”€ ğŸ¤– models.py                  # LLM model classes and Elo tracking
â”œâ”€â”€ âš”ï¸ matches.py                 # Match logic and prompt templates
â”œâ”€â”€ ğŸ—„ï¸ database.py                # MongoDB operations and data persistence
â”œâ”€â”€ âš™ï¸ config.py                  # Configuration and environment variables
â”œâ”€â”€ ğŸ“‹ model_definitions.py       # Model specifications and pricing
â”œâ”€â”€ ğŸ“Š leaderboard.py             # Results display and ranking
â”œâ”€â”€ ğŸ” check_models.py            # Model health checks and validation
â”œâ”€â”€ ğŸ“ˆ match_results_table.py     # Results analysis and visualization
â”œâ”€â”€ ğŸ”§ logger_config.py           # Logging configuration
â”œâ”€â”€ ğŸ“ logs/                      # Tournament logs and match history
â”œâ”€â”€ ğŸ§ª tests/                     # Unit tests and integration tests
â””â”€â”€ ğŸ“š docs/                      # Additional documentation
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/ -v

# Check code style
black . && flake8 . && mypy .
```


## ğŸ“„ License

This project is licensed under the ([![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
).

## ğŸ™ Acknowledgments

- Built with [LiteLLM](https://github.com/BerriAI/litellm) for unified LLM access
- Inspired by chess Elo rating systems and [TrueSkill](https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/)
- Mathematical framework based on [Bradley-Terry models](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)
- Clinical evaluation methodology inspired by medical education assessment
- Special thanks to the open-source AI research community
---

<div align="center">

</div>
