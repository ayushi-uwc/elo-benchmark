# 🏆 UNER: United Nasscom Elo Rating

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-sa/4.0/)
[![MongoDB](https://img.shields.io/badge/Database-MongoDB-green.svg)](https://www.mongodb.com/)
[![LiteLLM](https://img.shields.io/badge/LLM-LiteLLM-orange.svg)](https://github.com/BerriAI/litellm)

*An advanced peer-federated evaluation framework for large language models*

[🚀 Quick Start](#-quick-start) • [📖 How It Works](#-how-it-works) • [🔧 Installation](#-installation) • [🤝 Contributing](#-contributing)

</div>

---

## 🎯 Overview

**UNER** is a revolutionary evaluation framework that solves two critical problems in LLM assessment:

- 🎪 **Benchmark Overfitting**: Traditional static benchmarks become stale as models game them
- 💰 **Cost Blindness**: Performance metrics ignore real-world computational costs

### ✨ Key Innovations

| Feature | Description |
|---------|-------------|
| 🔄 **Dynamic Challenge Generation** | Fresh prompts generated by top models each cycle |
| ⚖️ **Peer-Federated Judging** | Models judge each other with ELO-weighted votes |
| 📊 **Dual-Track ELO Ratings** | Separate ratings for quality and cost-efficiency |
| 🏅 **Swiss Tournament Pairing** | Fair matchups within ELO windows |
| 📝 **Immutable Logging** | Complete transparency and reproducibility |

## 🚀 Quick Start

### Prerequisites
- Python 3.8+
- MongoDB database
- API keys for LLM providers

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys and MongoDB URI
```

### Run Your First Tournament

```bash
# Start a tournament with default settings
python main.py

# Run with custom parameters
python main.py --batch-size 10 --stats

# Check model health
python check_models.py
```

## 📊 How It Works

### 🏆 ELO Rating System

UNER maintains **two separate ELO ratings** for each model:

#### Raw Performance Rating
Based purely on judge votes and match outcomes

#### Cost-Adjusted Rating  
Incorporates computational costs into performance evaluation

### 🎭 Match Structure

1. **🎯 Model Pairing**: Swiss-style pairing within ELO windows
2. **📝 Case Generation**: Top model creates clinical scenario
3. **❓ Question Generation**: Another top model poses question
4. **💭 Response Collection**: Competing models answer
5. **⚖️ Judgment**: Peer models evaluate responses
6. **📈 ELO Updates**: Ratings adjusted based on weighted votes

## 🛠️ Configuration

### Environment Variables

Create a `.env` file with your credentials:

```env
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_API_KEY=your_google_key
HUGGINGFACE_API_KEY=your_huggingface_key
GROQ_API_KEY=your_groq_key
XAI_API_KEY=your_xai_key

# Database
MONGODB_URI=your_mongodb_connection_string
```

### Adding New Models

Edit `model_definitions.py`:

```python
{
    "name": "GPT-4 Turbo",
    "model_id": "gpt-4-turbo-preview", 
    "provider": "openai",
    "input_cost_per_million": 10.0,
    "output_cost_per_million": 30.0,
    "pricing_source": "OpenAI API Pricing"
}
```

## 📁 Project Structure

```
elo-benchmark/
├── 📄 main.py              # Main entry point
├── 🏆 tournament.py        # Tournament management
├── 🤖 models.py            # LLM model classes
├── ⚔️ matches.py           # Match logic & prompts
├── 🗄️ database.py          # MongoDB operations
├── ⚙️ config.py            # Configuration
├── 📋 model_definitions.py # Model specifications
├── 📊 leaderboard.py       # Results display
├── 🔍 check_models.py      # Health checks
└── 📝 logs/               # Tournament logs
```

## 🎮 Command Line Options

| Option | Description | Example |
|--------|-------------|---------|
| `--max-matches` | Maximum matches to run | `--max-matches 100` |
| `--batch-size` | Matches per batch | `--batch-size 10` |
| `--stats` | Show detailed statistics | `--stats` |
| `--stats-only` | Only show stats, no matches | `--stats-only` |
| `--log-level` | Set logging verbosity | `--log-level DEBUG` |

## 📈 Sample Output

```
DETAILED LEADERBOARD
┌───┬──────────────────┬────────┬────────┬──────────┬──────────┬────────┬────────┬───────────┬────────┐
│ # │Model             │Raw ELO │Cost ELO│Raw Avg   │Cost Avg  │W-L-D   │Tokens  │Cost $     │Matches │
├───┼──────────────────┼────────┼────────┼──────────┼──────────┼────────┼────────┼───────────┼────────┤
│ 1 │GPT-4 Turbo       │  1687.2│  1654.1│    0.6234│    0.5987│  12-8-2│   45231│  $0.12456│      22│
│ 2 │Claude-3 Opus     │  1623.8│  1598.7│    0.5876│    0.5654│  10-9-3│   38976│  $0.09876│      22│
│ 3 │Gemini Pro        │  1534.5│  1567.9│    0.5123│    0.5345│   8-12-2│   32145│  $0.06543│      22│
└───┴──────────────────┴────────┴────────┴──────────┴──────────┴────────┴────────┴───────────┴────────┘
```

## 🔬 Methodology Deep Dive

<details>
<summary>Click to expand detailed methodology</summary>

### 1. Model Registration
- Initialize metadata and baseline ELO (1500)
- Configure cost tracking and API endpoints

### 2. Fair Matching  
- Swiss-style pairing within ELO strata
- Prevent repeated matchups
- Balance computational costs

### 3. Challenge Creation
- Top models generate novel prompts
- Committee validation (80% approval threshold)
- Domain-agnostic framework

### 4. Solution Generation
- Standardized decoding parameters
- Comprehensive cost tracking
- Latency measurement

### 5. Peer Evaluation
- ELO-weighted judge selection
- Softmax vote weighting
- Binary outcome determination

### 6. Dual Scoring
- Raw performance calculation
- Cost-adjusted score computation
- ELO rating updates

### 7. Rating Propagation
- Gaussian kernel smoothing
- Cohesive rating evolution
- Stability maintenance

### 8. Logging & Dissemination
- Immutable audit trail
- Periodic leaderboards
- Performance analytics

</details>

## 🤝 Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/

# Check code style
black . && flake8 .
```

## 📄 License

This project is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](LICENSE).

## 🙏 Acknowledgments

- Built with [LiteLLM](https://github.com/BerriAI/litellm) for unified LLM access
- Inspired by chess ELO rating systems
- Special thanks to the open-source AI community

## 📞 Support

- 🐛 [Report Issues](https://github.com/yourusername/elo-benchmark/issues)
- 💬 [Discussions](https://github.com/yourusername/elo-benchmark/discussions)
- 📧 Email: support@example.com

---

<div align="center">

**⭐ Star this repo if you find it useful! ⭐**

Made with ❤️ by the UNER team

</div>